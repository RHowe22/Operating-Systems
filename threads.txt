			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Harshita Girase <hvgirase@buffalo.edu>
Douglas Achkar 	<dachkar@buffalo.edu>
Robert Howe	<rjhowe@buffalo.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


       Added to Struct Thread:
        int64_t wakeUpTime:                 // Time that a sleeping thread should wake up.

       Added to file thread.c:
        static struct sleeping_threads      // List that will hold sleeping threads.

       Added to file timer.c:
        bool sleeping_list_modified         // Bool that signals that timer_sleep is modifing the sleeping threads list
        struct semaphore thread_in_sleep    // Semaphore that handles threads that want to sleep


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

    After calling timer_sleep, the thread will be handled by sema_down(),
    which will prevent race conditions from cuncurrent threads trying to access timer_sleep.
    Then, it sets the sleeping_list_modified flag to prevent the 
    interrupt from modifying the list while timer_sleep is accessing it.
    Then, we will set the current thread's wake up time to be the value of 
    timer_ticks + ticks passed in as an argument in timer_sleep.
    Now, we will block the current thread and add it to the sleeping list in an ordered manner.
    After this, we can call sema_up and reset sleeping_list_modified to its default state.


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

    The sleeping thread list will be ordered by wake up time, where
 head points to the earliest wake up time thread(FIFO variation),
 minimizing total time spent in the interrupt handler.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

    By using sema_down on thread_in_sleep, we guarantee that only
    one thread at a time is accessing timer_sleep.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
  
  By using the sleeping_list_modified bool, we prevent both the 
interrupt and timer_sleep from modifying the list at the same time.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
 
   It's fairly simple and versatile. It has a simple functionality and
 won't affect other portions of the operating system, 
 like other implementations risked doing.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
 
	in thread.c in struct thread
	struct thread * donor  //is the thread that donated its priority
	                       // to this  thread

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
                    THREAD 1        THREAD 2          THREAD 3          THREAD 4
                 +----------+     +-----------+     +-----------+     +----------+
                 |PR    =42 |     |PR    =42  |     |PR    =42  |     |PR    =42 |
                 |DONOR ={} |---->|DONOR =1   |---->|DONOR =2   |---->|DONOR =3  |----> ...
                 +----------+     +-----------+     +-----------+     +----------+
PR = Priority
----> = donated priority


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

We ensure as whenever an element is blocked using semadown(), which is used to
 implement blocking threads in the other synch structures, it is added to its waiting list in
 order from the highest priority thread in the front and the lowest priority thread in the 
 back. If there is a tie between two threads priority, they are ordered by First-In-First-Out method" 

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When a thread tries to acquire a lock and it recognizes that t the lock is aquired,
it will check to see if its priority is greater than the lock holders. If it
is, it will donate its priority to the lockholder and block itself, otherwise it gets placed
on the waiting list in the lock.

If the lock holder is given a priority by another thread, it will set its donor to  point to this
thread and calling get priorirty on this thread will return the value of the donors thread.
If the donor had already been donated then add the previous donors to the waiters list in 
in the lock.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release is called on a lock that a higher priority thread is waiting
the thread holding the lock removes the thread donor it has if it has any and places it
back on the the waiting list in the lock and allows sema_up() to select the next highest
priority thread in this lock's waiting list to hold the lock.


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

- Let's say a thread is trying to update to a new priority. If at any point, there is 
an interrupt when the thread is trying to change its priority, a race condition can be
 created.  We call thread_get_priority_of() on the threads so we can get the updated value
 of the threads at every instance.  

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

- This design allows for multiple nested priority donations. since we 
use pointers to indicate donation, it basically results in a single linked list
that describes the thread's donation relationships. It is superior to our previous 
design idea of using an actual list, since the dimensions are not limited while using pointers.


			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

	in thread.c in struct thread
	int nice	 // the value of how nice this thread is
	fixed_point_t recent_cpu 	// this threads recent cpu time

	global variables
	stuct list [64] priority_queues // list of ready queue that store
	// threads with priority x in priority_queues[x]
	//threads in  higher indexes of priorty_queues will run first

	fixed_point_t load_average // estimated average number 
	// of threads ready to run in the last minute

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59       A
 4      4   0   0  62  61  59       A
 8      8   0   0  61  61  59       B
12      8   4   0  61  60  59       A
16      12  4   0  60  60  59       B
20      12  8   0  60  59  59       A
24      16  8   0  59  59  59       C
28      16  8   4  59  59  58       B
32      16 12   4  59  58  58       A
36      20 12   4  58  58  58       C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?
    Yes, for example at 16 ticks, when the priorities of A & B are tied at 60. There, we have the
    choice of letting A continue its execution until another process has a higher priority OR
    we can preempt A and let B run, considering that they have the same priority and threads that
    previously ran are added to a FCFS queue.
        We used the preemption rule, as it is the way our scheduler is implemented.


>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

The current thread's priority updates every 4 ticks since it is the only thread that changes its recent_cpu() value while the other threads' values remain constant. If there are a lot of threads in the system, this scheduling algorithm might affect performance negatively since we are calculating the load average, priority and recent_cpu() values for all threads every second. 

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

	the main critique behind our design is that it operates slowly
as it can spend much time in the timer interput handler doing calculations
and cheching conditions. if we had extra time look into making the size of
a list a memeber to save time calculating the number of threads ready
and connecting priority queues in the array in a way such that once the
highest priority queue becomes empty it goes to the next populated queue without
checking through all the empty queues in betweeen.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

In this assignment, we used the fixed point implementation that was provided
for us to allow us to use to find solutions that required real numbers.
That is why load_average and recent_cpu were both fixed because
they had to  be represented as real numbers. We only used these fixed
point values in functions that required us to use them as such.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

